import os  # For file and environment variable management
from dotenv import load_dotenv  # For loading environment variables
import chromadb  # For embedding-based storage and retrieval
from openai import OpenAI  # For interacting with OpenAI API
from chromadb.utils import embedding_functions  # For embedding function utilities

"""
This script demonstrates a pipeline for building a question-answering system using 
ChromaDB and OpenAI's GPT-3.5-turbo model for semantic search and response generation.

### Purpose:
1. **Document Storage and Embeddings**:
   - Load documents from a directory.
   - Split documents into manageable text chunks.
   - Generate vector embeddings for each chunk using OpenAI's embedding model.
   - Store these chunks in ChromaDB for persistent embedding-based retrieval.

2. **Query and Response Generation**:
   - Retrieve relevant document chunks based on a query using ChromaDB.
   - Generate a concise answer using GPT-3.5-turbo based on retrieved context.

### Key Functions:
1. `load_documents_from_directory`: Load `.txt` files into a list of dictionaries.
2. `split_text`: Split text documents into overlapping chunks.
3. `get_openai_embedding`: Generate embeddings for text chunks using OpenAI API.
4. `query_documents`: Retrieve relevant document chunks from ChromaDB based on a query.
5. `generate_response`: Use GPT-3.5-turbo to generate a concise answer from retrieved context.

### Inputs:
- Documents stored in the specified directory (e.g., `./news_articles`).
- A user query string for the question-answering system.

### Outputs:
- The final answer generated by GPT-3.5-turbo.
- Relevant document chunks retrieved from the database.

### Requirements:
- ChromaDB installed and configured for persistent storage.
- An OpenAI API key stored in a `.env` file.
"""

# Load environment variables from a .env file
load_dotenv()

# Fetch the OpenAI API key from the environment variables
openai_key = os.getenv("OPENAI_API_KEY")

# Initialize OpenAI embedding function for generating document embeddings
openai_ef = embedding_functions.OpenAIEmbeddingFunction(api_key = openai_key, 
                      model_name = "text-embedding-3-small"  # Specify the embedding model
    )

# Initialize the ChromaDB client with persistent storage for embeddings
chroma_client = chromadb.PersistentClient(path="chroma_persistent_storage")

# Define a collection name to organize stored embeddings
collection_name = "document_qa_collection"

# Retrieve or create a ChromaDB collection for storing embeddings
collection = chroma_client.get_or_create_collection(
    name=collection_name, 
    embedding_function=openai_ef
)

# Initialize the OpenAI client to interact with GPT models
client = OpenAI(api_key=openai_key)

# Create a chat completion example using GPT-3.5-turbo
resp = client.chat.completions.create(
    model="gpt-3.5-turbo",  # Specify the GPT model
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},  # Define assistant behavior
        {"role": "user", "content": "What is human life expectancy in the United States?"}  # User query
    ],
).to_dict()  # Convert the response to a dictionary for easier access

# Function to load documents from a specified directory
def load_documents_from_directory(directory_path: str) -> list:
    """
    Loads all text files from the specified directory into a list of dictionaries.

    Args:
        directory_path (str): Path to the directory containing `.txt` files.

    Returns:
        list: A list of dictionaries, where each dictionary contains:
              - 'id' (str): The filename of the document.
              - 'text' (str): The content of the document.
    """
    print("==== Loading documents from directory ====")
    documents = []
    # Iterate through each file in the directory
    for filename in os.listdir(directory_path):
        # Only process `.txt` files
        if filename.endswith(".txt"):
            # Open the file and read its content
            with open(os.path.join(directory_path, filename), "r", encoding="utf-8") as file:
                documents.append({"id": filename, "text": file.read()})  # Append file data
    return documents

# Function to split text into smaller overlapping chunks
def split_text(text, chunk_size=1000, chunk_overlap=50):
    """
    Splits text into smaller chunks with a specified overlap.

    Args:
        text (str): The text to be split.
        chunk_size (int): The number of characters in each chunk.
        chunk_overlap (int): The number of overlapping characters between chunks.

    Returns:
        list: A list of text chunks.
    """
    chunks = []
    start = 0  # Initialize the starting index
    # Loop to create chunks until the text is fully processed
    while start < len(text):
        end = start + chunk_size  # Determine the end of the chunk
        chunks.append(text[start:end])  # Append the chunk to the list
        start = end - chunk_overlap  # Adjust the start for the next chunk
    return chunks

# Load documents from the specified directory
directory_path = "./news_articles"  # Path to documents
documents = load_documents_from_directory(directory_path)
print(f"Loaded {len(documents)} documents")

# Split each document into smaller chunks
chunked_documents = []
for doc in documents:
    chunks = split_text(doc["text"])  # Split the document into chunks
    for i, chunk in enumerate(chunks):
        chunked_documents.append({"id": f"{doc['id']}_chunk{i+1}", "text": chunk})  # Append chunk with ID
print(f"Split documents into {len(chunked_documents)} chunks")

# Function to generate embeddings for a text using OpenAI API
def get_openai_embedding(text):
    """
    Generates an embedding for the given text using OpenAI's embedding model.

    Args:
        text (str): The text to be embedded.

    Returns:
        list: A vector representation of the text.
    """
    response = client.embeddings.create(
        input=text, 
        model="text-embedding-3-small"  # Specify the embedding model
    )
    embedding = response.data[0].embedding  # Extract the embedding vector
    return embedding

# Generate embeddings for all document chunks and upsert into ChromaDB
for doc in chunked_documents:
    doc["embedding"] = get_openai_embedding(doc["text"])  # Generate embeddings
    collection.upsert(  # Insert the chunk into the database
        ids=[doc["id"]], 
        documents=[doc["text"]], 
        embeddings=[doc["embedding"]]
    )

# Function to query documents based on a user question
def query_documents(question, n_results=2):
    """
    Queries ChromaDB to find relevant document chunks for a given question.

    Args:
        question (str): The user query.
        n_results (int): Number of top results to retrieve.

    Returns:
        list: Relevant document chunks matching the query.
    """
    results = collection.query(query_texts=question, n_results=n_results)
    # Flatten the list of results into a single list of relevant chunks
    relevant_chunks = [doc for sublist in results["documents"] for doc in sublist]
    return relevant_chunks

# Function to generate a GPT response based on retrieved context
def generate_response(question, relevant_chunks):
    """
    Generates a concise response to a question based on retrieved context.

    Args:
        question (str): The user query.
        relevant_chunks (list): Contextual document chunks.

    Returns:
        str: A concise answer to the question.
    """
    context = "\n\n".join(relevant_chunks)  # Combine context into a single string
    prompt = (
        "You are an assistant for question-answering tasks. Use the following pieces of "
        "retrieved context to answer the question. If you don't know the answer, say that you "
        "don't know. Use three sentences maximum and keep the answer concise."
        "\n\nContext:\n" + context + "\n\nQuestion:\n" + question
    )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo", 
        messages=[
            {"role": "system", "content": prompt},  # Provide the context and prompt
            {"role": "user", "content": question}  # Provide the user question
        ]
    )
    return response.choices[0].message.content  # Return the assistant's answer

# Example query and response generation
question = "Tell me about Databricks"
relevant_chunks = query_documents(question)  # Retrieve relevant document chunks
answer = generate_response(question, relevant_chunks)  # Generate a response
print(answer)  # Print the answer
